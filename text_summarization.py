# -*- coding: utf-8 -*-
"""Text_Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSbgfaYYbVuYgtrIRd8RtNQhQrgGiClm
"""

pip install transformers datasets torch nltk

from datasets import load_dataset

# Load CNN/DailyMail dataset

dataset = load_dataset("cnn_dailymail", "3.0.0")

# Print an example article

article = dataset['train'][0]['article'] # display first article

print("Original Article:\n", article)

from transformers import BartForConditionalGeneration, BartTokenizer

# Load the BART tokenizer and model

tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

# Select an article and reference summary

article = dataset["test"][0]["article"]

reference_summary = dataset["test"][0]["highlights"]

# Tokenize input text

inputs = tokenizer(article, return_tensors="pt", truncation=True, max_length=1024)

# Generate summary

summary_ids = model.generate(inputs["input_ids"], num_beams=4, min_length=30, max_length=150, early_stopping=True)

generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Print summaries

print("Reference Summary:\n", reference_summary)

print("\nGenerated Summary:\n", generated_summary)

pip install rouge-score

from rouge_score import rouge_scorer

# Initialize ROUGE scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# Calculate ROUGE scores

scores = scorer.score(reference_summary, generated_summary)

# Print scores

print("\nROUGE Scores:")

for key, value in scores.items():

    print(f"{key}: Precision: {value.precision:.4f}, Recall: {value.recall:.4f}, F1-score: {value.fmeasure:.4f}")

pip install bert-score

from bert_score import score

# Calculate BERT scores
# Wrap the summary strings in lists to make them iterable for the score function
P, R, F1 = score([generated_summary], [reference_summary], lang="en", verbose=True)
model_type = "bert-base-uncased"

# Print scores

print("\nBERT Scores:")

print(f"Precision: {P.mean():.2f}, Recall: {R.mean():.2f}, F1-score: {F1.mean():.2f}")

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load Pretrained Model and Tokenizer
model_name = "t5-small"  # You can use t5-base, t5-large, etc.
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Input Text
text = """The T5 model is a transformer-based machine learning model developed by Google Research.
It is trained on a diverse dataset and performs multiple NLP tasks by converting them into text-to-text problems."""

# Prepend "summarize:" as required by T5
input_text = "summarize: " + text
input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

# Generate Summary
summary_ids = model.generate(input_ids, max_length=50, num_beams=5, length_penalty=2.0, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(summary)

from datasets import load_dataset

# Load CNN/Daily Mail dataset

dataset = load_dataset("cnn_dailymail", "3.0.0")

# Split dataset

train_data = dataset["train"]

val_data = dataset["validation"]

test_data = dataset["test"]

